{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d725159-2e1f-4daf-ac77-d25105d3d7db",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'src'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[1;32m     14\u001b[0m warnings\u001b[38;5;241m.\u001b[39mfilterwarnings(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     17\u001b[0m     standardize_player_name, standardize_team_name, get_column_mapping,\n\u001b[1;32m     18\u001b[0m     clean_numeric_column, print_data_summary\n\u001b[1;32m     19\u001b[0m )\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mload_receiving_data\u001b[39m() \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame:\n\u001b[1;32m     22\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"load and combine all receiving statistics files (rec20XX.csv)\"\"\"\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'src'"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "data integration and cleaning module for nfl wide receiver rookie prediction\n",
    "combines all raw csv files, handles missing values, and creates unified dataset\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import sys\n",
    "sys.path.append('/home/ubuntu')\n",
    "from typing import List, Dict, Tuple\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from src.utils import (\n",
    "    standardize_player_name, standardize_team_name, get_column_mapping,\n",
    "    clean_numeric_column, print_data_summary\n",
    ")\n",
    "\n",
    "def load_receiving_data() -> pd.DataFrame:\n",
    "    \"\"\"load and combine all receiving statistics files (rec20XX.csv)\"\"\"\n",
    "    print(\"loading receiving statistics data...\")\n",
    "    \n",
    "    rec_files = glob.glob('/home/ubuntu/Uploads/rec20*.csv')\n",
    "    rec_files.sort()\n",
    "    \n",
    "    all_rec_data = []\n",
    "    \n",
    "    for file in rec_files:\n",
    "        year = int(file.split('rec')[1].split('.')[0])\n",
    "        print(f\"  processing {file} (year: {year})\")\n",
    "        \n",
    "        try:\n",
    "            df = pd.read_csv(file)\n",
    "            df['season'] = year\n",
    "            \n",
    "            # standardize column names\n",
    "            column_mapping = get_column_mapping()\n",
    "            df = df.rename(columns=column_mapping)\n",
    "            \n",
    "            # clean player names and team names\n",
    "            if 'player_name' in df.columns:\n",
    "                df['player_name'] = df['player_name'].apply(standardize_player_name)\n",
    "            if 'team' in df.columns:\n",
    "                df['team'] = df['team'].apply(standardize_team_name)\n",
    "            \n",
    "            all_rec_data.append(df)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  error loading {file}: {e}\")\n",
    "    \n",
    "    if all_rec_data:\n",
    "        combined_rec = pd.concat(all_rec_data, ignore_index=True, sort=False)\n",
    "        print(f\"combined receiving data: {combined_rec.shape}\")\n",
    "        return combined_rec\n",
    "    else:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def load_rookie_data() -> pd.DataFrame:\n",
    "    \"\"\"load and combine all rookie draft information files\"\"\"\n",
    "    print(\"loading rookie draft data...\")\n",
    "    \n",
    "    rookie_files = glob.glob('/home/ubuntu/Uploads/*rookie*.csv')\n",
    "    rookie_files.extend(glob.glob('/home/ubuntu/Uploads/20*rookies.csv'))\n",
    "    rookie_files = list(set(rookie_files))  # remove duplicates\n",
    "    rookie_files.sort()\n",
    "    \n",
    "    all_rookie_data = []\n",
    "    \n",
    "    for file in rookie_files:\n",
    "        print(f\"  processing {file}\")\n",
    "        \n",
    "        try:\n",
    "            # try different encodings\n",
    "            df = None\n",
    "            for encoding in ['utf-8', 'latin-1', 'cp1252']:\n",
    "                try:\n",
    "                    df = pd.read_csv(file, encoding=encoding)\n",
    "                    break\n",
    "                except UnicodeDecodeError:\n",
    "                    continue\n",
    "            \n",
    "            if df is None:\n",
    "                print(f\"  could not read {file} with any encoding\")\n",
    "                continue\n",
    "            \n",
    "            # extract year from filename\n",
    "            year_match = None\n",
    "            for part in file.split('/'):\n",
    "                if any(char.isdigit() for char in part):\n",
    "                    year_str = ''.join(filter(str.isdigit, part))\n",
    "                    if len(year_str) == 4 and year_str.startswith('20'):\n",
    "                        year_match = int(year_str)\n",
    "                        break\n",
    "            \n",
    "            if year_match:\n",
    "                df['rookie_year'] = year_match\n",
    "            \n",
    "            # standardize column names\n",
    "            column_mapping = get_column_mapping()\n",
    "            df = df.rename(columns=column_mapping)\n",
    "            \n",
    "            # filter for wide receivers only\n",
    "            if 'Pos' in df.columns:\n",
    "                df = df[df['Pos'] == 'WR'].copy()\n",
    "            elif 'position' in df.columns:\n",
    "                df = df[df['position'] == 'WR'].copy()\n",
    "            \n",
    "            # clean player names and team names\n",
    "            if 'player_name' in df.columns:\n",
    "                df['player_name'] = df['player_name'].apply(standardize_player_name)\n",
    "            if 'team' in df.columns:\n",
    "                df['team'] = df['team'].apply(standardize_team_name)\n",
    "            \n",
    "            all_rookie_data.append(df)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  error loading {file}: {e}\")\n",
    "    \n",
    "    if all_rookie_data:\n",
    "        combined_rookies = pd.concat(all_rookie_data, ignore_index=True, sort=False)\n",
    "        \n",
    "        # remove duplicates based on player name and rookie year\n",
    "        if 'player_name' in combined_rookies.columns and 'rookie_year' in combined_rookies.columns:\n",
    "            combined_rookies = combined_rookies.drop_duplicates(\n",
    "                subset=['player_name', 'rookie_year'], keep='first'\n",
    "            )\n",
    "        \n",
    "        print(f\"combined rookie data: {combined_rookies.shape}\")\n",
    "        return combined_rookies\n",
    "    else:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def load_advanced_metrics() -> pd.DataFrame:\n",
    "    \"\"\"load and combine advanced wr metrics files\"\"\"\n",
    "    print(\"loading advanced metrics data...\")\n",
    "    \n",
    "    advanced_files = glob.glob('/home/ubuntu/Uploads/nfl-advanced-wr-*.csv')\n",
    "    advanced_files.sort()\n",
    "    \n",
    "    all_advanced_data = []\n",
    "    \n",
    "    for file in advanced_files:\n",
    "        print(f\"  processing {file}\")\n",
    "        \n",
    "        try:\n",
    "            df = pd.read_csv(file)\n",
    "            \n",
    "            # extract year from filename\n",
    "            year_match = None\n",
    "            for part in file.split('-'):\n",
    "                if part.isdigit() and len(part) == 4:\n",
    "                    year_match = int(part.split('.')[0])\n",
    "                    break\n",
    "            \n",
    "            if year_match:\n",
    "                df['season'] = year_match\n",
    "            \n",
    "            # standardize column names\n",
    "            column_mapping = get_column_mapping()\n",
    "            df = df.rename(columns=column_mapping)\n",
    "            \n",
    "            # clean player names and team names\n",
    "            if 'player' in df.columns:\n",
    "                df['player_name'] = df['player'].apply(standardize_player_name)\n",
    "            if 'team' in df.columns:\n",
    "                df['team'] = df['team'].apply(standardize_team_name)\n",
    "            \n",
    "            all_advanced_data.append(df)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  error loading {file}: {e}\")\n",
    "    \n",
    "    if all_advanced_data:\n",
    "        combined_advanced = pd.concat(all_advanced_data, ignore_index=True, sort=False)\n",
    "        print(f\"combined advanced metrics: {combined_advanced.shape}\")\n",
    "        return combined_advanced\n",
    "    else:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def load_target_data() -> pd.DataFrame:\n",
    "    \"\"\"load 1000+ yard seasons target data\"\"\"\n",
    "    print(\"loading target variable data...\")\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_csv('/home/ubuntu/Uploads/1kseasons.csv')\n",
    "        \n",
    "        # standardize column names\n",
    "        df = df.rename(columns={\n",
    "            'Player': 'player_name',\n",
    "            'Rookie year': 'rookie_year',\n",
    "            '1,000 Yard Seasons': 'thousand_yard_seasons'\n",
    "        })\n",
    "        \n",
    "        # clean player names\n",
    "        df['player_name'] = df['player_name'].apply(standardize_player_name)\n",
    "        \n",
    "        # clean numeric columns\n",
    "        df['rookie_year'] = clean_numeric_column(df['rookie_year'])\n",
    "        df['thousand_yard_seasons'] = clean_numeric_column(df['thousand_yard_seasons'])\n",
    "        \n",
    "        # create binary target variable\n",
    "        df['has_1000_yard_season'] = (df['thousand_yard_seasons'] > 0).astype(int)\n",
    "        \n",
    "        print(f\"target data loaded: {df.shape}\")\n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"error loading target data: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def merge_all_datasets(rec_data: pd.DataFrame, rookie_data: pd.DataFrame, \n",
    "                      advanced_data: pd.DataFrame, target_data: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"merge all datasets into unified dataframe\"\"\"\n",
    "    print(\"merging all datasets...\")\n",
    "    \n",
    "    # start with rookie data as base\n",
    "    merged_df = rookie_data.copy()\n",
    "    print(f\"starting with rookie data: {merged_df.shape}\")\n",
    "    \n",
    "    # merge with receiving data for rookie seasons\n",
    "    if not rec_data.empty and 'player_name' in merged_df.columns and 'rookie_year' in merged_df.columns:\n",
    "        # create merge key for rookie seasons\n",
    "        rec_data_rookie = rec_data.copy()\n",
    "        if 'player_name' in rec_data_rookie.columns and 'season' in rec_data_rookie.columns:\n",
    "            rec_data_rookie['merge_key'] = rec_data_rookie['player_name'] + '_' + rec_data_rookie['season'].astype(str)\n",
    "            \n",
    "            merged_df['merge_key'] = merged_df['player_name'] + '_' + merged_df['rookie_year'].astype(str)\n",
    "            \n",
    "            merged_df = pd.merge(\n",
    "                merged_df, rec_data_rookie,\n",
    "                on='merge_key', \n",
    "                how='left',\n",
    "                suffixes=('', '_rec')\n",
    "            )\n",
    "            merged_df = merged_df.drop('merge_key', axis=1)\n",
    "            print(f\"after merging receiving data: {merged_df.shape}\")\n",
    "    \n",
    "    # merge with advanced metrics\n",
    "    if not advanced_data.empty and 'player_name' in merged_df.columns and 'rookie_year' in merged_df.columns:\n",
    "        # create merge key for advanced metrics\n",
    "        advanced_data_copy = advanced_data.copy()\n",
    "        if 'player_name' in advanced_data_copy.columns and 'season' in advanced_data_copy.columns:\n",
    "            advanced_data_copy['merge_key'] = advanced_data_copy['player_name'] + '_' + advanced_data_copy['season'].astype(str)\n",
    "            \n",
    "            merged_df['merge_key'] = merged_df['player_name'] + '_' + merged_df['rookie_year'].astype(str)\n",
    "            \n",
    "            merged_df = pd.merge(\n",
    "                merged_df, advanced_data_copy,\n",
    "                on='merge_key',\n",
    "                how='left',\n",
    "                suffixes=('', '_adv')\n",
    "            )\n",
    "            merged_df = merged_df.drop('merge_key', axis=1)\n",
    "            print(f\"after merging advanced metrics: {merged_df.shape}\")\n",
    "        else:\n",
    "            print(\"advanced metrics missing required columns for merge\")\n",
    "    \n",
    "    # merge with target data\n",
    "    if not target_data.empty and 'player_name' in merged_df.columns and 'rookie_year' in merged_df.columns:\n",
    "        target_cols = ['player_name', 'rookie_year', 'has_1000_yard_season', 'thousand_yard_seasons']\n",
    "        available_target_cols = [col for col in target_cols if col in target_data.columns]\n",
    "        \n",
    "        merged_df = pd.merge(\n",
    "            merged_df, target_data[available_target_cols],\n",
    "            on=['player_name', 'rookie_year'],\n",
    "            how='left'\n",
    "        )\n",
    "        print(f\"after merging target data: {merged_df.shape}\")\n",
    "    \n",
    "    return merged_df\n",
    "\n",
    "def clean_merged_dataset(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"comprehensive cleaning of merged dataset\"\"\"\n",
    "    print(\"cleaning merged dataset...\")\n",
    "    \n",
    "    df_clean = df.copy()\n",
    "    \n",
    "    # remove rows with missing player names\n",
    "    if 'player_name' in df_clean.columns:\n",
    "        df_clean = df_clean.dropna(subset=['player_name'])\n",
    "    \n",
    "    # clean numeric columns\n",
    "    numeric_columns = [\n",
    "        'draft_round', 'draft_pick', 'age', 'rec', 'rec_yards', 'rec_td',\n",
    "        'targets', 'routes_run', 'air_yards', 'yac', 'target_share',\n",
    "        'route_participation', 'contested_catch_pct'\n",
    "    ]\n",
    "    \n",
    "    for col in numeric_columns:\n",
    "        if col in df_clean.columns:\n",
    "            df_clean[col] = clean_numeric_column(df_clean[col])\n",
    "    \n",
    "    # handle missing target variable\n",
    "    if 'has_1000_yard_season' not in df_clean.columns:\n",
    "        df_clean['has_1000_yard_season'] = 0\n",
    "    else:\n",
    "        df_clean['has_1000_yard_season'] = df_clean['has_1000_yard_season'].fillna(0)\n",
    "    \n",
    "    # create derived features\n",
    "    if 'rec' in df_clean.columns and 'targets' in df_clean.columns:\n",
    "        df_clean['catch_rate'] = np.where(\n",
    "            df_clean['targets'] > 0,\n",
    "            df_clean['rec'] / df_clean['targets'],\n",
    "            0\n",
    "        )\n",
    "    \n",
    "    if 'rec_yards' in df_clean.columns and 'rec' in df_clean.columns:\n",
    "        df_clean['yards_per_reception'] = np.where(\n",
    "            df_clean['rec'] > 0,\n",
    "            df_clean['rec_yards'] / df_clean['rec'],\n",
    "            0\n",
    "        )\n",
    "    \n",
    "    if 'rec_yards' in df_clean.columns and 'routes_run' in df_clean.columns:\n",
    "        df_clean['yards_per_route_run'] = np.where(\n",
    "            df_clean['routes_run'] > 0,\n",
    "            df_clean['rec_yards'] / df_clean['routes_run'],\n",
    "            0\n",
    "        )\n",
    "    \n",
    "    # remove duplicate rows\n",
    "    if 'player_name' in df_clean.columns and 'rookie_year' in df_clean.columns:\n",
    "        df_clean = df_clean.drop_duplicates(subset=['player_name', 'rookie_year'], keep='first')\n",
    "    \n",
    "    print(f\"cleaned dataset shape: {df_clean.shape}\")\n",
    "    return df_clean\n",
    "\n",
    "def main():\n",
    "    \"\"\"main function to execute data integration pipeline\"\"\"\n",
    "    print(\"starting nfl wr rookie prediction data integration pipeline\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # load all datasets\n",
    "    rec_data = load_receiving_data()\n",
    "    rookie_data = load_rookie_data()\n",
    "    advanced_data = load_advanced_metrics()\n",
    "    target_data = load_target_data()\n",
    "    \n",
    "    # debug: print column names\n",
    "    print(\"\\ndebugging column names:\")\n",
    "    print(f\"rookie data columns: {list(rookie_data.columns) if not rookie_data.empty else 'empty'}\")\n",
    "    print(f\"advanced data columns: {list(advanced_data.columns) if not advanced_data.empty else 'empty'}\")\n",
    "    \n",
    "    # merge datasets\n",
    "    merged_data = merge_all_datasets(rec_data, rookie_data, advanced_data, target_data)\n",
    "    \n",
    "    # clean merged dataset\n",
    "    clean_data = clean_merged_dataset(merged_data)\n",
    "    \n",
    "    # print summary statistics\n",
    "    print_data_summary(clean_data, \"final cleaned dataset\")\n",
    "    \n",
    "    # save cleaned dataset\n",
    "    output_path = '/home/ubuntu/outputs/cleaned_dataset.parquet'\n",
    "    clean_data.to_parquet(output_path, index=False)\n",
    "    print(f\"\\ncleaned dataset saved to: {output_path}\")\n",
    "    \n",
    "    # save csv version for inspection\n",
    "    csv_path = '/home/ubuntu/outputs/cleaned_dataset.csv'\n",
    "    clean_data.to_csv(csv_path, index=False)\n",
    "    print(f\"csv version saved to: {csv_path}\")\n",
    "    \n",
    "    # create data profile report\n",
    "    create_data_profile(clean_data)\n",
    "    \n",
    "    print(\"\\ndata integration pipeline completed successfully!\")\n",
    "\n",
    "def create_data_profile(df: pd.DataFrame):\n",
    "    \"\"\"create basic data profiling report\"\"\"\n",
    "    print(\"creating data profile report...\")\n",
    "    \n",
    "    profile_lines = []\n",
    "    profile_lines.append(\"# nfl wr rookie prediction - data profile report\\n\")\n",
    "    profile_lines.append(f\"**dataset shape:** {df.shape[0]} rows, {df.shape[1]} columns\\n\")\n",
    "    profile_lines.append(f\"**memory usage:** {df.memory_usage(deep=True).sum() / 1024**2:.2f} mb\\n\")\n",
    "    \n",
    "    # target variable distribution\n",
    "    if 'has_1000_yard_season' in df.columns:\n",
    "        target_dist = df['has_1000_yard_season'].value_counts()\n",
    "        profile_lines.append(\"\\n## target variable distribution\\n\")\n",
    "        profile_lines.append(f\"- no 1000+ yard seasons: {target_dist.get(0, 0)} ({target_dist.get(0, 0)/len(df)*100:.1f}%)\\n\")\n",
    "        profile_lines.append(f\"- has 1000+ yard seasons: {target_dist.get(1, 0)} ({target_dist.get(1, 0)/len(df)*100:.1f}%)\\n\")\n",
    "    \n",
    "    # missing values summary\n",
    "    missing = df.isnull().sum()\n",
    "    missing_pct = (missing / len(df)) * 100\n",
    "    missing_df = pd.DataFrame({\n",
    "        'missing_count': missing,\n",
    "        'missing_percent': missing_pct\n",
    "    }).sort_values('missing_count', ascending=False)\n",
    "    \n",
    "    profile_lines.append(\"\\n## missing values summary\\n\")\n",
    "    for col, row in missing_df[missing_df['missing_count'] > 0].iterrows():\n",
    "        profile_lines.append(f\"- {col}: {row['missing_count']} ({row['missing_percent']:.1f}%)\\n\")\n",
    "    \n",
    "    # numeric columns summary\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    if len(numeric_cols) > 0:\n",
    "        profile_lines.append(\"\\n## numeric columns summary\\n\")\n",
    "        desc = df[numeric_cols].describe()\n",
    "        profile_lines.append(desc.to_string())\n",
    "        profile_lines.append(\"\\n\")\n",
    "    \n",
    "    # save profile report\n",
    "    profile_path = '/home/ubuntu/outputs/data_profile_report.md'\n",
    "    with open(profile_path, 'w') as f:\n",
    "        f.writelines(profile_lines)\n",
    "    \n",
    "    print(f\"data profile report saved to: {profile_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c8b018-1ce0-47fb-befc-0dcf5267ea1a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
